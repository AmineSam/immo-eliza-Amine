# Kaggle Hyperparameter Tuning - Quick Start Guide

## Files to Upload to Kaggle

1. **`data_for_kaggle.csv`** - Your pre-processed dataset (generated by `export_for_kaggle.py`)
2. **`kaggle_optuna_tuning.py`** - The tuning script

## Setup on Kaggle

### 1. Create a New Notebook
- Go to Kaggle → Notebooks → New Notebook
- Choose "Script" (not "Notebook")

### 2. Upload Dataset
- Create a new dataset with `data_for_kaggle.csv`
- Add it as input to your notebook

### 3. Update DATA_PATH
In `kaggle_optuna_tuning.py`, line 23:
```python
DATA_PATH = "/kaggle/input/your-dataset-name/data_for_kaggle.csv"
```
Replace `your-dataset-name` with your actual dataset name.

### 4. Install Dependencies
Kaggle has most packages pre-installed, but add this at the top if needed:
```python
!pip install optuna catboost -q
```

### 5. Run the Script
```python
python kaggle_optuna_tuning.py
```

## What Gets Saved

All files are saved to `/kaggle/working/` (automatically downloadable):

| File | Description | Size |
|------|-------------|------|
| `stage3_fitted_params.pkl` | Feature engineering parameters | ~1 MB |
| `study_xgb.pkl` | XGBoost Optuna study | ~5 MB |
| `study_lgb.pkl` | LightGBM Optuna study | ~5 MB |
| `study_cat.pkl` | CatBoost Optuna study | ~5 MB |
| `model_xgb.pkl` | Trained XGBoost model | ~10 MB |
| `model_lgb.pkl` | Trained LightGBM model | ~10 MB |
| `model_cat.pkl` | Trained CatBoost model | ~10 MB |
| `model_ensemble.pkl` | Stacking ensemble | ~30 MB |
| `best_params.pkl` | Best hyperparameters | <1 MB |
| `model_results.csv` | Performance metrics | <1 KB |

**Total: ~80 MB** (easily downloadable from Kaggle)

## Configuration Options

### Adjust Number of Trials
```python
N_TRIALS = 100  # Default
# For faster tuning: N_TRIALS = 50
# For better results: N_TRIALS = 200
```

### Adjust Train/Test Split
```python
TEST_SIZE = 0.15  # 85/15 split (recommended)
# Alternative: TEST_SIZE = 0.20  # 80/20 split
```

## Expected Runtime on Kaggle

With **GPU enabled** (recommended):
- XGBoost: ~15-20 minutes (100 trials)
- LightGBM: ~10-15 minutes (100 trials)
- CatBoost: ~20-25 minutes (100 trials)
- Ensemble training: ~5 minutes
- **Total: ~50-65 minutes**

With **CPU only**:
- Total: ~2-3 hours

## Using the Saved Models

### Load and Use Models Locally
```python
import pickle

# Load ensemble model
with open('model_ensemble.pkl', 'rb') as f:
    ensemble = pickle.load(f)

# Load feature engineering parameters
with open('stage3_fitted_params.pkl', 'rb') as f:
    fitted_params = pickle.load(f)

# Make predictions on new data
# (after applying same Stage 3 transformations)
predictions = ensemble.predict(X_new)
```

### Load Best Parameters
```python
with open('best_params.pkl', 'rb') as f:
    params = pickle.load(f)

print("XGBoost params:", params['xgboost'])
print("LightGBM params:", params['lightgbm'])
print("CatBoost params:", params['catboost'])
```

## Tips for Kaggle

1. **Enable GPU**: Settings → Accelerator → GPU P100
2. **Increase timeout**: Settings → Session timeout → 12 hours
3. **Save intermediate results**: The script saves after each model
4. **Monitor progress**: Optuna shows progress bars
5. **Download all files**: After completion, download from Output tab

## Troubleshooting

**If you run out of memory:**
- Reduce `N_TRIALS` to 50
- Use smaller `cv` folds in ensemble (change `cv=5` to `cv=3`)

**If it's too slow:**
- Enable GPU
- Reduce `N_TRIALS` to 50
- Use `MedianPruner` in Optuna

**If results don't meet target:**
- Increase `N_TRIALS` to 200
- Adjust hyperparameter ranges in objective functions
- Try different `TEST_SIZE` values

## Next Steps After Tuning

1. Download all `.pkl` files from Kaggle
2. Review `model_results.csv` to see which model performed best
3. Use the best model or ensemble for final predictions
4. Optionally: Re-train with full dataset (no test split) for production
